{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2c0abd51",
      "metadata": {
        "id": "2c0abd51"
      },
      "source": [
        "# Feedforward classification of the NMIST data\n",
        "### Advanced Deep Learning 2024\n",
        "This notebook was written originally Jon Sporring (mailto:sporring@di.ku.dk) and heavily inspired by https://clay-atlas.com/us/blog/2021/04/22/pytorch-en-tutorial-4-train-a-model-to-classify-mnist.\n",
        "\n",
        "We consider the Modified National Institute of Standards and Technology database of handwritten digits (MNIST): http://yann.lecun.com/exdb/mnist/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4b1cd80",
      "metadata": {
        "id": "a4b1cd80"
      },
      "source": [
        "## Installs\n",
        "\n",
        "On non-colab system, is usually good to make an environment and install necessary tools there. E.g., anaconda->jupyter->terminal create an environment, if you have not already, and activate it:\n",
        "```\n",
        "conda create -n adl python=3.9\n",
        "conda activate adl\n",
        "```\n",
        "then install missing packages such as:\n",
        "```\n",
        "conda install ipykernel torch matplotlib torchmetrics scikit-image jpeg\n",
        "conda install -c conda-forge segmentation-models-pytorch ipywidgets\n",
        "```\n",
        "and if you want to add it to jupyter's drop-down menu\n",
        "```\n",
        "ipython kernel install --user --name=adl\n",
        "```\n",
        "Now reload the jupyter-notebook's homepage and make a new or load an existing file. On colab, the tools have to be installed everytime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4afe90fa",
      "metadata": {
        "id": "4afe90fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc5e2a9-91c1-4638-94f4-b31be889f0ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cpu)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.13.1)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2024.9.20)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (0.4)\n",
            "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface-hub>=0.24.6 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.26.2)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (1.16.0)\n",
            "Collecting timm==0.9.7 (from segmentation-models-pytorch)\n",
            "  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.20.1+cpu)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.66.6)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (6.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (0.4.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.8.30)\n",
            "Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=fac880482e88b832a8a9154eb90e404f8d740ba3590749695f106838cf11e604\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=fa2b60f267237fd6200a21c7aac155fd5581c6192bb62dd9c8fd8464b9e38ef1\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, lightning-utilities, torchmetrics, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 lightning-utilities-0.11.9 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.4 timm-0.9.7 torchmetrics-1.6.0\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "if IN_COLAB:\n",
        "    !pip3 install torch matplotlib torchmetrics scikit-image segmentation-models-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0112f199",
      "metadata": {
        "id": "0112f199"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "372f3da4",
      "metadata": {
        "id": "372f3da4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as dset\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "709fa153",
      "metadata": {
        "id": "709fa153"
      },
      "source": [
        "## Set global device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef384f3a",
      "metadata": {
        "id": "ef384f3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6bd1881-28be-453d-99c0-99f684c289c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU State: cpu\n"
          ]
        }
      ],
      "source": [
        "# GPU\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print('GPU State:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0479fa7",
      "metadata": {
        "id": "c0479fa7"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d728ef",
      "metadata": {
        "id": "16d728ef"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, loss, optimizer, loader, epochs, verbose=True, device=device):\n",
        "    \"\"\"\n",
        "    Run training of a model given a loss function, optimizer and a set of training and validation data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for times, data in enumerate(loader):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            inputs = inputs.view(inputs.shape[0], -1)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Foward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            loss_tensor = loss(outputs, labels)\n",
        "            loss_tensor.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics\n",
        "            running_loss += loss_tensor.item()\n",
        "            if verbose:\n",
        "                if times % 100 == 99 or times+1 == len(loader):\n",
        "                    print('[%d/%d, %d/%d] loss: %.3f' % (epoch+1, epochs, times+1, len(loader), running_loss/2000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d7ce08f",
      "metadata": {
        "id": "2d7ce08f"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, loader, device=device):\n",
        "    \"\"\"\n",
        "    Evaluate a model 'model' on all batches of a torch DataLoader 'data_loader'.\n",
        "\n",
        "    Returns: the total number of correct classifications,\n",
        "             the total number of images\n",
        "             the list of the per class correct classification,\n",
        "             the list of the per class total number of images.\n",
        "    \"\"\"\n",
        "\n",
        "    # Test\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            inputs = inputs.view(inputs.shape[0], -1)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    class_correct = [0 for i in range(10)]\n",
        "    class_total = [0 for i in range(10)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            inputs = inputs.view(inputs.shape[0], -1)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            c = (predicted == labels).squeeze()\n",
        "            for i in range(len(labels)):\n",
        "                label = labels[i]\n",
        "                class_correct[label] += c[i].item()\n",
        "                class_total[label] += 1\n",
        "\n",
        "    return (correct, total, class_correct, class_total)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd353eee",
      "metadata": {
        "id": "cd353eee"
      },
      "source": [
        "## Main program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "142ae598",
      "metadata": {
        "id": "142ae598"
      },
      "outputs": [],
      "source": [
        "# Transform\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,), (0.5,)),]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94591bc3",
      "metadata": {
        "id": "94591bc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acb19a8d-9b2e-4b98-b116-47a19ea1f183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 32.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST/MNIST/raw/train-images-idx3-ubyte.gz to MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.23MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.29MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.47MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Data\n",
        "trainSet = datasets.MNIST(root='MNIST', download=True, train=True, transform=transform)\n",
        "testSet = datasets.MNIST(root='MNIST', download=True, train=False, transform=transform)\n",
        "trainLoader = dset.DataLoader(trainSet, batch_size=64, shuffle=True)\n",
        "testLoader = dset.DataLoader(testSet, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "848e7dcc",
      "metadata": {
        "id": "848e7dcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92f4acde-7ef9-4168-e4d1-21c895eb14e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "    (5): LogSoftmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(in_features=784, out_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=128, out_features=64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=64, out_features=10),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "\n",
        "net = Net().to(device)\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fce0735b",
      "metadata": {
        "id": "fce0735b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63019cf4-95c5-422b-f7ee-684c30ca6211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 60000 images\n",
            "[1/4, 100/938] loss: 0.109\n",
            "[1/4, 200/938] loss: 0.182\n",
            "[1/4, 300/938] loss: 0.223\n",
            "[1/4, 400/938] loss: 0.252\n",
            "[1/4, 500/938] loss: 0.276\n",
            "[1/4, 600/938] loss: 0.298\n",
            "[1/4, 700/938] loss: 0.318\n",
            "[1/4, 800/938] loss: 0.337\n",
            "[1/4, 900/938] loss: 0.355\n",
            "[1/4, 938/938] loss: 0.362\n",
            "[2/4, 100/938] loss: 0.016\n",
            "[2/4, 200/938] loss: 0.033\n",
            "[2/4, 300/938] loss: 0.050\n",
            "[2/4, 400/938] loss: 0.066\n",
            "[2/4, 500/938] loss: 0.082\n",
            "[2/4, 600/938] loss: 0.097\n",
            "[2/4, 700/938] loss: 0.113\n",
            "[2/4, 800/938] loss: 0.128\n",
            "[2/4, 900/938] loss: 0.143\n",
            "[2/4, 938/938] loss: 0.148\n",
            "[3/4, 100/938] loss: 0.013\n",
            "[3/4, 200/938] loss: 0.027\n",
            "[3/4, 300/938] loss: 0.040\n",
            "[3/4, 400/938] loss: 0.054\n",
            "[3/4, 500/938] loss: 0.066\n",
            "[3/4, 600/938] loss: 0.079\n",
            "[3/4, 700/938] loss: 0.091\n",
            "[3/4, 800/938] loss: 0.104\n",
            "[3/4, 900/938] loss: 0.116\n",
            "[3/4, 938/938] loss: 0.121\n",
            "[4/4, 100/938] loss: 0.012\n",
            "[4/4, 300/938] loss: 0.035\n",
            "[4/4, 400/938] loss: 0.046\n",
            "[4/4, 500/938] loss: 0.057\n",
            "[4/4, 600/938] loss: 0.067\n",
            "[4/4, 700/938] loss: 0.078\n",
            "[4/4, 800/938] loss: 0.088\n",
            "[4/4, 900/938] loss: 0.099\n",
            "[4/4, 938/938] loss: 0.102\n",
            "Training Finished.\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 94 %\n",
            "Accuracy of 0: 0.978571\n",
            "Accuracy of 1: 0.985903\n",
            "Accuracy of 2: 0.950581\n",
            "Accuracy of 3: 0.941584\n",
            "Accuracy of 4: 0.938900\n",
            "Accuracy of 5: 0.920404\n",
            "Accuracy of 6: 0.965553\n",
            "Accuracy of 7: 0.928988\n",
            "Accuracy of 8: 0.899384\n",
            "Accuracy of 9: 0.918731\n"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "epochs = 4\n",
        "lr = 0.002\n",
        "loss = nn.NLLLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.002, momentum=0.9)\n",
        "\n",
        "# Train\n",
        "print('Training on %d images' % trainSet.data.shape[0])\n",
        "training_loop(net, loss, optimizer, trainLoader, epochs)\n",
        "print('Training Finished.\\n')\n",
        "\n",
        "# Test\n",
        "correct, total, class_correct, class_total = evaluate_model(net, testLoader)\n",
        "print('Accuracy of the network on the %d test images: %d %%' % (testSet.data.shape[0], (100*correct / total)))\n",
        "for i in range(10):\n",
        "    print('Accuracy of %d: %3f' % (i, (class_correct[i]/class_total[i])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f06850a",
      "metadata": {
        "id": "7f06850a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "adl",
      "language": "python",
      "name": "adl"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}